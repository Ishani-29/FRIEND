{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f52cdaf4-d78e-4785-9ea5-bcc73c2ce652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9c0f150-7f84-4bd4-802c-23509b92b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import features, load_dataset\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "import torch\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1de0b15b-261d-41b1-bca9-e92d1307c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"zera09/finance_dpo_split\", split=\"train\")\n",
    "dataset = dataset.filter(lambda x: os.path.exists('/home/sarmistha/Sarmistha/ECML/' + x[\"image_path\"]))\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('dataset_with_context_ada2_final1.csv')\n",
    "\n",
    "\n",
    "# Create a dictionary mapping queries to retrieved_text_context\n",
    "query_to_context = dict(zip(df['query'], df['retrieved_text_context']))\n",
    "\n",
    "# Function to add retrieved_text_context based on prompt\n",
    "def add_context(example):\n",
    "    example[\"retrieved_text_context\"] = query_to_context.get(example[\"query\"], \"\")  # Use \"prompt\" instead of \"query\"\n",
    "    return example\n",
    "\n",
    "# Apply the function to the dataset\n",
    "dataset = dataset.map(add_context)\n",
    "\n",
    "# Verify the new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8a543c4-e406-4357-910b-038c07dbbe01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query', 'chosen', 'rejected', 'image_path', 'retrieved_text_context'],\n",
       "    num_rows: 441\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29f613f5-caa7-4fb4-93ae-5a780112c2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700a35c0436746c594db67f1ba984efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\",device_map='auto')\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
    "adapter_path = \"zera09/Qwen2.5\"\n",
    "model.load_adapter(adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49ba06a4-e4be-4cd0-91bf-dfa029cc1b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 1,843,200 | All params: 3,756,466,176 | Trainable%: 0.05%\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"lora_\" in name:  # All LoRA parameters\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False  # Freeze base model\n",
    "def print_trainable_params(model):\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            #print(f\"Trainable: {name} | Shape: {param.shape} | Dtype: {param.dtype}\")\n",
    "    print(f\"Trainable params: {trainable_params:,} | All params: {all_params:,} | Trainable%: {100 * trainable_params / all_params:.2f}%\")\n",
    "\n",
    "print_trainable_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2bef240-dbbb-4f57-9b21-9f34bf23b002",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# def format(example):\n",
    "#     # Prepare the input for the chat template\n",
    "#     path = example['image']\n",
    "#     if  os.path.exists(path):\n",
    "#         prompt = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": example[\"query\"]}]}]\n",
    "#     else:\n",
    "#         prompt = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": example[\"query\"]}]}]\n",
    "    \n",
    "#     chosen = [{\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": example[\"answer\"]}]}]\n",
    "#     rejected = [{\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": example[\"answer\"]}]}]\n",
    "#     # Apply the chat template\n",
    "#     prompt = processor.apply_chat_template(prompt, tokenize=False)\n",
    "#     chosen = processor.apply_chat_template(chosen, tokenize=False)\n",
    "#     rejected = processor.apply_chat_template(rejected, tokenize=False)\n",
    "#     # Resize the image to ensure it fits within the maximum allowable\n",
    "#     # size of the processor to prevent OOM errors.\n",
    "#     #max_size = processor.image_processor.size[\"longest_edge\"] // 2\n",
    "#     #example[\"image\"].thumbnail((max_size, max_size))\n",
    "#     if os.path.exists(path):\n",
    "#         image = Image.open(path).resize((255,255))\n",
    "#     else:\n",
    "#         image = None\n",
    "#     return {\"images\": [image], \"prompt\": prompt, \"chosen\": chosen, \"rejected\": rejected}\n",
    "\n",
    "# Apply the formatting function to the dataset\n",
    "# dataset = dataset.map(format, remove_columns=dataset.column_names, num_proc=32)\n",
    "\n",
    "from PIL import Image\n",
    "def format(example):\n",
    "    # Prepare the input for the chat template\n",
    "    path = '/home/sarmistha/Sarmistha/ECML/'+example['image_path']\n",
    "    \n",
    "    # # if  os.path.exists(path):\n",
    "    # prompt1 = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": example[\"query\"]}]},\n",
    "    #          {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": example[\"chosen\"]}]}]\n",
    "    \n",
    "    \n",
    "    # prompt2 = [{\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": example[\"query\"]}]},\n",
    "    #          {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": example[\"rejected\"]}]}]\n",
    "    \n",
    "    #     # Apply the chat template\n",
    "    # p1 = processor.apply_chat_template(prompt1, tokenize=False)\n",
    "    # p2 = processor.apply_chat_template(prompt2, tokenize=False)\n",
    "    # #print(p1)\n",
    "    # # print(p2)\n",
    "    # prompt = p1.split(\"\"\"<start_of_turn>model\"\"\")[0]\n",
    "    \n",
    "    # chosen = \"<start_of_turn>model\"+ p1.split(\"\"\"<start_of_turn>model\"\"\")[1]\n",
    "    # # print(chosen)\n",
    "    # rejected = \"<start_of_turn>model\"+ p2.split(\"\"\"<start_of_turn>model\"\"\")[1]\n",
    "       \n",
    "    #print(type(prompt))\n",
    "    image = Image.open(path).resize((255,255))\n",
    "\n",
    "    #return {\"images\": [image], \"prompt\": example[\"query\"], \"chosen\":example[\"chosen\"], \"rejected\": example[\"rejected\"]}\n",
    "   \n",
    "    return {\"images\": [image], \"prompt\":'<image>'+example[\"query\"]+' This is the Context:'+example['retrieved_text_context'], \"chosen\":example[\"chosen\"], \"rejected\": example[\"rejected\"]}\n",
    "\n",
    "    \n",
    "\n",
    "# Apply the formatting function to the dataset\n",
    "dataset = dataset.map(format, remove_columns=dataset.column_names, num_proc=2)\n",
    "#ds = dataset.map(format, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eaf165b-dc58-4897-9b37-5c30b62cce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = dataset.features\n",
    "f[\"images\"] = features.Sequence(features.Image(decode=True))\n",
    "dataset = dataset.cast(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "651ad32f-0b3c-49ab-8cfd-61fcedb2192c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1756157/1417894479.py:12: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `DPOTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = DPOTrainer(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzeramarveenlyngkhoi\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/sarmistha/Sarmistha/ACM_MM/Priya/wandb/run-20250411_204045-blrw6uoo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/zeramarveenlyngkhoi/huggingface/runs/blrw6uoo' target=\"_blank\">qwen-dpo_rag_context_v3</a></strong> to <a href='https://wandb.ai/zeramarveenlyngkhoi/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/zeramarveenlyngkhoi/huggingface' target=\"_blank\">https://wandb.ai/zeramarveenlyngkhoi/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/zeramarveenlyngkhoi/huggingface/runs/blrw6uoo' target=\"_blank\">https://wandb.ai/zeramarveenlyngkhoi/huggingface/runs/blrw6uoo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='220' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [220/220 09:55, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.688300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.677400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.671100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.658900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.651900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.643900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.624900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.613100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.619900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.609100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.566200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.591800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.582500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.579400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.598100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.578100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.562700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.571900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.572400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.573800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.570400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=220, training_loss=0.6107189525257457, metrics={'train_runtime': 603.1692, 'train_samples_per_second': 1.462, 'train_steps_per_second': 0.365, 'total_flos': 0.0, 'train_loss': 0.6107189525257457, 'epoch': 1.9864253393665159})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = DPOConfig(\n",
    "        output_dir=\"qwen-dpo_rag_context_v3\",\n",
    "        #bf16=True,\n",
    "        gradient_checkpointing=True,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_train_epochs=2,\n",
    "        #dataset_num_proc=2,  # tokenization will use 32 processes\n",
    "        dataloader_num_workers=2,  # data loading will use 32 workers\n",
    "        logging_steps=10,\n",
    "    )\n",
    "trainer = DPOTrainer(\n",
    "        model,\n",
    "        #ref_model=ref_model,  # not needed when using peft\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        tokenizer=processor,\n",
    "        #peft_config=LoraConfig(target_modules=[\"q_proj\", \"v_proj\"]),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac6a875d-0d50-49da-9790-921d6af336e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d6c2fee79e4a1c833d063d00e008df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbe074bedb2424d8171aa80eb0ca89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1744378843.dgx01.1756157.0:   0%|          | 0.00/22.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822c3dae04214bcba6492fa5812c0aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/7.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa812d9f13f4a81817db32e6959648a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f58699345ee4a2793243192ad8bb254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/6.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/zera09/qwen-dpo_rag_context_v3/commit/3588b6769427770b9ea0db842e41e0388fd483b7', commit_message='End of training', commit_description='', oid='3588b6769427770b9ea0db842e41e0388fd483b7', pr_url=None, repo_url=RepoUrl('https://huggingface.co/zera09/qwen-dpo_rag_context_v3', endpoint='https://huggingface.co', repo_type='model', repo_id='zera09/qwen-dpo_rag_context_v3'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login('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')\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c737db-4558-49d2-aff6-93b468def739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (slomvlm)",
   "language": "python",
   "name": "slomvlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
