{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c7268e2-374e-4f0b-9d4b-8770e1ad3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df58a6fc-d553-47c9-9a4a-7c04452d50dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login('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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c440b1f-db8e-4382-8e80-3512c8db9d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"zera09/finance_chat\", split = \"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb596d39-ca81-4668-b993-bb8283edf664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0262d71e-a45a-4250-a84a-e80247ae16bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def format_data(sample):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": 'You are a finacnial assistant.'}],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\":Image.open('/home/sarmistha/Sarmistha/ECML/'+sample['image_path']).resize((255,255)),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": sample[\"query\"],\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": sample['answer']}],\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cc8c23f-7393-4ec7-bf10-fef108db82a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89292ba4289b47b7b00108c7b211bcb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1645 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee0b06c4edcf4900809196081f527bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/412 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "train_dataset = [ ]\n",
    "for sample in tqdm(dataset['train']):\n",
    "    if os.path.exists('/home/sarmistha/Sarmistha/ECML/'+sample['image_path']):\n",
    "        train_dataset.append(format_data(sample))\n",
    "eval_dataset = []\n",
    "\n",
    "for sample in tqdm(dataset['test']):\n",
    "    if os.path.exists('/home/sarmistha/Sarmistha/ECML/'+sample['image_path']):\n",
    "        eval_dataset.append(format_data(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169e30b9-6264-422b-b2cf-1518ff9aa3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a58ae03-bea7-438f-9791-d9e3e544ced2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4897454c6184ba3a3b13ce90cf4f0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "\n",
    "model_id = \"google/gemma-3-4b-it\"\n",
    "model =Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a78bb5bf-2409-4e0b-8429-35963174ce2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from qwen_vl_utils import process_vision_info\n",
    "\n",
    "\n",
    "# def generate_text_from_sample(model, processor, sample, max_new_tokens=1024, device=\"cuda\"):\n",
    "#     # Prepare the text input by applying the chat template\n",
    "#     text_input = processor.apply_chat_template(\n",
    "#         sample[0:2], tokenize=False, add_generation_prompt=True  # Use the sample without the system message\n",
    "#     )\n",
    "\n",
    "#     # Process the visual input from the sample\n",
    "#     image_inputs, _ = process_vision_info(sample)\n",
    "\n",
    "#     # Prepare the inputs for the model\n",
    "#     model_inputs = processor(\n",
    "#         text=[text_input],\n",
    "#         images=image_inputs,\n",
    "#         return_tensors=\"pt\",\n",
    "#     ).to(\n",
    "#         device\n",
    "#     )  # Move inputs to the specified device\n",
    "\n",
    "#     # Generate text with the model\n",
    "#     generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n",
    "\n",
    "#     # Trim the generated ids to remove the input ids\n",
    "#     trimmed_generated_ids = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "\n",
    "#     # Decode the output text\n",
    "#     output_text = processor.batch_decode(\n",
    "#         trimmed_generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "#     )\n",
    "\n",
    "#     return output_text[0]  # Return the first decoded output text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9778088c-d6d0-4129-9d4b-0f65f6443b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of how to call the method with sample:\n",
    "# output = generate_text_from_sample(model, processor, train_dataset[0])\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "457a89cf-0183-421a-ad4e-c3567ba011e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU allocated memory: 0.00 GB\n",
      "GPU reserved memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import time\n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if \"inputs\" in globals():\n",
    "        del globals()[\"inputs\"]\n",
    "    if \"model\" in globals():\n",
    "        del globals()[\"model\"]\n",
    "    if \"processor\" in globals():\n",
    "        del globals()[\"processor\"]\n",
    "    if \"trainer\" in globals():\n",
    "        del globals()[\"trainer\"]\n",
    "    if \"peft_model\" in globals():\n",
    "        del globals()[\"peft_model\"]\n",
    "    if \"bnb_config\" in globals():\n",
    "        del globals()[\"bnb_config\"]\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8035dd9b-46fd-4119-86e8-e2ab0addf1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199ff6fe8a324ccb935e1196a0a15076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=bnb_config\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60a67706-c18f-4432-a0fd-8a26fed227c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,223,552 || all params: 4,303,303,024 || trainable%: 0.0749\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply PEFT model adaptation\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff25a84a-21f8-4aed-a359-e1c9c77559ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "# Configure training arguments\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"gemma3-fin-chat\",  # Directory to save the model\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    per_device_train_batch_size=4,  # Batch size for training\n",
    "    per_device_eval_batch_size=4,  # Batch size for evaluation\n",
    "    gradient_accumulation_steps=8,  # Steps to accumulate gradients\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing for memory efficiency\n",
    "    # Optimizer and scheduler settings\n",
    "    optim=\"adamw_torch_fused\",  # Optimizer type\n",
    "    learning_rate=2e-4,  # Learning rate for training\n",
    "    lr_scheduler_type=\"constant\",  # Type of learning rate scheduler\n",
    "    # Logging and evaluation\n",
    "    logging_steps=10,  # Steps interval for logging\n",
    "    eval_steps=10,  # Steps interval for evaluation\n",
    "    eval_strategy=\"steps\",  # Strategy for evaluation\n",
    "    save_strategy=\"steps\",  # Strategy for saving the model\n",
    "    save_steps=20,  # Steps interval for saving\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to evaluate the best model\n",
    "    greater_is_better=False,  # Whether higher metric values are better\n",
    "    load_best_model_at_end=True,  # Load the best model after training\n",
    "    # Mixed precision and gradient settings\n",
    "    #bf16=True,  # Use bfloat16 precision\n",
    "    tf32=True,  # Use TensorFloat-32 precision\n",
    "    max_grad_norm=0.3,  # Maximum norm for gradient clipping\n",
    "    warmup_ratio=0.03,  # Ratio of total steps for warmup\n",
    "    # Hub and reporting\n",
    "    push_to_hub=True,  # Whether to push model to Hugging Face Hub\n",
    "    #report_to=\"wandb\",  # Reporting tool for tracking metrics\n",
    "    # Gradient checkpointing settings\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Options for gradient checkpointing\n",
    "    # Dataset configuration\n",
    "    dataset_text_field=\"\",  # Text field in dataset\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},  # Additional dataset options\n",
    "    # max_seq_length=1024  # Maximum sequence length for input\n",
    ")\n",
    "\n",
    "training_args.remove_unused_columns = False  # Keep unused columns in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0464ade1-d7ff-410a-92d1-ecae34851259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data collator to encode text and image pairs\n",
    "def collate_fn(examples):\n",
    "    # Get the texts and images, and apply the chat template\n",
    "    texts = [\n",
    "        processor.apply_chat_template(example, tokenize=False) for example in examples\n",
    "    ]  # Prepare texts for processing\n",
    "    #image_inputs = [process_vision_info(example)[0] for example in examples]  # Process the images to extract inputs\n",
    "\n",
    "    # Tokenize the texts and process the images\n",
    "    batch = processor(\n",
    "        text=texts, return_tensors=\"pt\", padding=True\n",
    "    )  # Encode texts and images into tensors\n",
    "\n",
    "    # The labels are the input_ids, and we mask the padding tokens in the loss computation\n",
    "    labels = batch[\"input_ids\"].clone()  # Clone input IDs for labels\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100  # Mask padding tokens in labels\n",
    "\n",
    "    # Ignore the image token index in the loss computation (model specific)\n",
    "    if isinstance(model,  Gemma3ForConditionalGeneration):  # Check if the processor is Qwen2VLProcessor\n",
    "        image_tokens = [262144]  # Specific image token IDs for Qwen2VLProcessor\n",
    "    else:\n",
    "        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]  # Convert image token to ID\n",
    "\n",
    "    # Mask image token IDs in the labels\n",
    "    for image_token_id in image_tokens:\n",
    "        labels[labels == image_token_id] = -100  # Mask image token IDs in labels\n",
    "\n",
    "    batch[\"labels\"] = labels  # Add labels to the batch\n",
    "\n",
    "    return batch  # Return the prepared batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86f15530-09d5-464d-8d7a-dfafe155e78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3758697/93380869.py:3: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/sarmistha/miniconda3/envs/slomvlm/lib/python3.9/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/home/sarmistha/miniconda3/envs/slomvlm/lib/python3.9/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33063cfd-9526-4f72-a907-3fe561e2a4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c29f814d-46d3-46e3-9a83-56c52c9aa0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.config.use_cache = False\n",
    "model.config.text_config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10b1cfbd-6ef8-4891-92ea-9398e4c43be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.text_config.use_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64472376-5633-427a-b48e-4b7c5c58118b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='153' max='153' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [153/153 13:23, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>41.109400</td>\n",
       "      <td>4.064242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>28.870200</td>\n",
       "      <td>3.033843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>21.659900</td>\n",
       "      <td>2.370467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>18.045600</td>\n",
       "      <td>2.087451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>16.575100</td>\n",
       "      <td>1.926558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>14.093300</td>\n",
       "      <td>1.796920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>14.150000</td>\n",
       "      <td>1.655795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>12.974300</td>\n",
       "      <td>1.599896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>12.738000</td>\n",
       "      <td>1.567858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>12.480000</td>\n",
       "      <td>1.535935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>11.405700</td>\n",
       "      <td>1.514796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>12.028600</td>\n",
       "      <td>1.496572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>11.670400</td>\n",
       "      <td>1.480080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>11.529200</td>\n",
       "      <td>1.466038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>11.575100</td>\n",
       "      <td>1.454075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the best model at gemma3-fin-chat/checkpoint-150/pytorch_model.bin, if you are running a distributed training on multiple nodes, you should activate `--save_on_each_node`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=153, training_loss=16.616010429033267, metrics={'train_runtime': 806.7975, 'train_samples_per_second': 6.113, 'train_steps_per_second': 0.19, 'total_flos': 1.3693729473256704e+16, 'train_loss': 16.616010429033267})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae34d61-de43-4c46-b590-0ff845d348f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (slomvlm)",
   "language": "python",
   "name": "slomvlm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
